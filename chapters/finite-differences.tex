This section has been heavily extracted from some notes from the WIAS in Berlin. They didn't have any authorship, so I don't really know who to thank for them.  Probably the simplest (and oldest) idea for approximating equations stems from the use of Taylor's theorem:

\begin{theorem}\label{thm:taylor}
    Consider a function $f$ in $C^n((a,b), \R)$, i.e. $n$ times differentiable whose derivatives are well-defined in $a$. Then, there is some $\xi$ in $(a,b)$ such that
    $$ f(b) = \sum_{k=0}^{n-1}\frac{f^{(k)}(a)}{k!}(b-a)^k + \frac{f^{(n)}(\xi)}{n!}(b-a)^n. $$
\end{theorem}
Naturally, this is the mean value theorem when $n=1$. From now on we consider our setting to be in $\R$. For any continuous function $f:(a,b)\to \R$, we can define its associated \emph{grid function} given by its evaluation in a finite set of points. We sometimes refer to this as \emph{collocation}. Set $a<b$, and define the points $a=x_1<\hdots<x_N=b$ with $h_i=x_{i}-x_{i-1}$, such that we have $N$ points, and define the grid function as the vector
    $$ R_hf = (f_1, \hdots, f_N) \coloneqq (f(x_1), \hdots, f(x_N)) \in \R^N,$$
    where we have defined the $R_h$ operator such that $R_hf:\R^N \to \R^N$ and $R_h:C((a,b),\R)\to [\R^N\to\R^N]$.  We can then define the following difference operators: 
    \begin{itemize}
        \item Forward difference: 
            $$ D^+f(x_i) \coloneqq \frac{f_{i+1} - f_i}{h_{i+1}} $$
        \item Backward difference: 
            $$ D^-f(x_i) \coloneqq \frac{f_{i} - f_{i-1}}{h_{i}} $$
        \item Centered difference:
            $$ Df(x_i) \coloneqq \frac{f_{i+1} - f_{i-1}}{h_{i+1} - h_{i-1}} $$

    \end{itemize}
A nice simple exercise is to extend these ideas to second order derivatives, which all stem from Taylor's formula. The application we will be interested is in that of approximating the action of a differential operator using these formulas. For this, we will focus on the forward difference formula. At each grid point we will have
$$ D^+f(x_i) = \frac{f_{i+1} - f_i}{h_{i+1}} = \frac 1{h_{i+1}} [-1\quad 1]\begin{bmatrix}f_i \\ f_{i+1}\end{bmatrix}, $$
so that defining the matrix $\mat D_h^+$ in $\R^{N\times N}$ as
$$ [\mat D_h^+]_{ij} = \begin{cases} -1/h_{i+1} & j=i \\ 1/h_i  & j=i+1 \\ 0 & \text{elsewhere} \end{cases} $$
we obtain an operator which, given a grid function, it yields its discrete derivative. Naturally, care has to be taken at the last point, where we can simply use instead a backwards difference. 

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{loglogaxis}[ 
            xlabel={$h$}, 
            ylabel={$\|y' - Dy\|_{\infty}$}, 
            legend style={at={(1.05,0.5)}, anchor=west, column sep=1ex, legend columns=1, align=left}, 
            grid=major,
            width=10cm, height=6cm, % Adjust the size if needed
            ]
            \addplot[
                blue, 
                line width=1pt
            ] table [x=h, y=fd, col sep=comma] {convergence_rates_FD.csv};
            
            \addplot[
                orange, 
                dashed, 
                line width=1pt
            ] table [x=h, y=bd, col sep=comma] {convergence_rates_FD.csv};
            
            \addplot[
                green, 
                line width=1pt
            ] table [x=h, y=cd, col sep=comma] {convergence_rates_FD.csv};

            % Legend
            \legend{forward, backward, centered}
        \end{loglogaxis}
    \end{tikzpicture}
    \caption{Convergence of forward, centered and backward difference schemes for calculating the first derivative of $u(x)=\sin(2\pi x)$. Note that the centered difference error starts increasing for $h\lesssim 10^{-6}$.}
    \label{fig:fd_convergence}
\end{figure}

Finite difference approximations can be easily seen to be convergent for sufficiently smooth functions. To see this, consider Taylor's theorem to obtain for $h>0$:
    $$ f(x+ h) = f(x) + f'(x)h + O(h^2), $$
from which we readily obtain that 
    $$ | D^+f(x) - f'(x) | = O(h), $$
    so that forward differences and also backward differences (straightforward calculation) converge pointwise linearly. However, it is noteworthy that although centered differences also start converging linearly, they become unstable for very small $h$, see  Figure \ref{fig:fd_convergence}.

\subsection{Second order derivatives}
The standard way of approximating second order derivatives is that of using a finite difference approximation twice. It is given by the following: 
$$ \begin{aligned}
    f''(x) &\approx D^+(f')(x) \\
           &=\frac{f'(x+h) - f'(x)}{h} \\
           &\approx\frac{\frac{f(x+h) - f(x)}{h} - \frac{f(x) - f(x-h)}{h}}{h} \\
           &= \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}.
    \end{aligned}
$$
We note that the intermediate steps show that these computations can be represented as the matrix product $\mat D^+ \mat D^-$, where the order of differentiation can be inverted and thus equivalently $\mat D^- \mat D^+$. 
\subsection{Order of convergence}
The order of convergence of the previously computed schemes can be derived from Taylor's theorem. For example, we previously showed that backward/forward differences converge linearly (pointwise), and indeed we can do the same for centered differences. For this, consider the following third order expansions:
    $$
    \begin{aligned}
        f(x+h) &= f(x) + hf'(x) + \frac {h^2} 2 f''(x) + \mathcal O(h^3) \\
        f(x-h) &= f(x) - hf'(x) + \frac {h^2} 2 f''(x) + \mathcal O(h^3),
    \end{aligned}
    $$
and by simply adding both equations it can be shown that the finite difference formula computed converges linearly.
\subsection{The empirical convergence rate}
One issue that is always overviewed is the computation of the \emph{observed} convergence rate, so we will write it here once and for all so that it is never an issue again. Assume that some error $\vec e_h$ converges to zero in some $X$ norm with rate $k$:
    $$ \|\vec e_h\|_X \leq C h^k.$$
    Then, given a sequence of mesh sizes $h$ and errors as $(h_j, [\vec e_h]_j)$, we would like to estimate $k$ to validate we really obtained the computed rate $k$. For this, we take the logarithm of the error expression:
    $$ \log \|[\vec e_h]_j\|_X \leq \log C + k \log h_j, $$
and note that we obtain roughly a line equation if we assume an equality. Because of this, we can estimate now estimate the slope of our line equation using two consecutive points as
$$ k \approx \frac{\log \|[\vec e_h]_{j+1}\|_X - \log \|[\vec e_h]_j\|_X}{ \log h_{j+1} - \log h_j}, $$
which yields the classical expression one gets everywhere: 
    $$ k \approx  \frac{\log \left(\|[\vec e_h]_{j+1}\|_X / \|[\vec e_h]_j\|_X\right)}{ \log \left(h_{j+1} / h_j\right)}. $$

    This is also referred to sometimes as \emph{numerical convergence rate}. 

\subsection{Method of manufactured solutions}
A typical problem is that one wants to estimate the convergence of some discrete solution
    $$ \mat L_h \vec u_h = 0, $$
which approximates a differential equation
    $$ \mathcal L u = 0 $$
    in which one typically does not know the analytical solution. This method allows to compute a modified problem in which we know the analytic solution. Indeed, consider an arbitrary analytic function (that we know) $u_{ex}$. The main idea here is that we can define a right hand side 
    $$ f_{ex} \coloneqq \mathcal L u_{ex}, $$
such that the solution of the modified problem 
    $$ \mathcal L u = f \quad ( = \mathcal L u_{ex}) $$
    is, if uniqueness holds, $u = u_{ex}$. To obtain the discrete problem, we consider the associated grid function $R_h u_{ex}$ and set the discrete right hand side $\vec f_h = R_h \mathcal L u_{ex}$ such that the solution of the discrete problem 
    $$ \mat L_h \vec u_h = \vec f_h \quad (=R_h \mathcal L u_{ex}) $$
converges to the continuous solution $u_{ex}$ by construction. 

\subsection{Convergence Analysis}
Using these ideas, we can now proceed to propose a mechanisms for discretizing differential operators. A central question regards the capability of approximation that such an objects has, which finds its answer in the Lax equivalence theorem. We will first require other definitions. 

\begin{definition}[Consistency]
    Consider some differential operator $\mathcal A$. A discrete operator $\mat A_h$ in $\R^{N\times N}$ is said to be \emph{consistent} with $\mathcal A$ of order $k$ if for all sufficiently smooth functions it holds that
    $$ \max_{i\in\{1,\hdots,N\}}|\mathcal Af(x_i) - [\mat A_hR_hf]_i| =: \|R_h \mathcal Af - \mat A_hR_h f\|_{\infty,h} \leq C h^k, $$
    where $\|\cdot \|_{\infty,h}$ is the induced infinity norm in $\R^N$ and $C$ is a positive constant independent of $h$. This is sometimes stated more weakly by saying that the norm is $\mathcal O(h^k)$. 
\end{definition}

\example{The difference operators are all consistent with the first order derivative, which can be seen immediately from Taylor's theorem.}
\begin{definition}[Stability]
    A finite difference operator $\mat A_h$ is said to be stable (in the discrete maximum norm) if there is a stability constant $C>0$ that does not depend on the grid size such that
        $$ \| \vec v_h \|_{\infty,h} \leq C \|\mat A_h \vec v_h \|_{\infty,h}$$
        for all grid functions $\vec v_h$. 
\end{definition}
We finally consider an infinite dimensional problem given by
    $$ \mathcal A u = f $$
and its finite difference approximation as
    $$ \mat A_h \vec u_h = \vec f_h, $$
    which will allow us to define convergence. 

\begin{definition}[Convergence]
    The finite differences scheme is said to be convergent of order $k$ in the discrete maximum norm if the discrete solution and the continuous one satisfy that 
    $$ \| \vec u_h - R_hu \|_{\infty,h} \leq C h^k. $$
This is sometimes stated more weakly by saying simply that $\| \vec u_h - R_hu \|_{\infty,h}$ is $\mathcal O(h^k)$. 
\end{definition}

\begin{theorem}[Lax equivalence theorem]
    Let $ \mat A_h \vec u_h = \vec f_h$ be the finite difference discretization for the problem $\mathcal A u = f$ as defined above. If $A_h$ is consistent, then stability and convergence are equivalent. 
    \begin{proof}
        The stability to convergence part is taken directly from the aforementionted notes. The convergence to stability is instead partly inspired by the notes by Long Chen on abstract convergence analysis \cite{chenLFDM}.
        
        \paragraph{Stability $\implies$ convergence:}

        We proceed directly through inequalities:
        \begin{align*}
            \| \vec u_h - R_h u\|_{\infty,h} &\leq C\|\mat A_h(\vec u_h - R_hu) \|_{\infty,h} && \text{(Stability)} \\
                                             &=C \| f_h - \mat A_hR_h u \|_{\infty,h} && \\
                                             &=C \| R_h f - \mat A_hR_h u\|_{\infty,h} &&\\
                                             &=C \|R_h(\mathcal A u) - \mat A_hR_h u\|_{\infty,h}&& \\
                                             &\leq Ch^k && \text{(Consistency)}.
        \end{align*}

        \paragraph{Convergence $\implies$ stability:}

        We note that by taking inverses, we can write stability as 
        $$ \| \mat A_h^{-1}\vec f_h \|_{\infty, h} \leq C \|\vec f_h\|_{\infty, h}, $$
        which is simply the continuity of the inverse map $\mat A_h^{-1}$. Consider an arbitrary grid vector $\vec f_h$, and we denote the associated problem solution as $\vec u_h$, where $\mat A_h \vec u_h = \vec f_h$. Using this, we have that
        $$ 
        \begin{aligned} 
            \|\mat A_h^{-1}\vec f_h\|_{\infty,h} &= \| \vec u_h \|_{\infty, h} \leq \| \vec u_h - R_h u \|_{\infty, h} + \| R_h u \|_{\infty, h} && \text{(Convergence)}\\
                                                 &\leq C h^k + \| R_h \mathcal A^{-1} f \|_{\infty, h}. 
        \end{aligned}
        $$
        To bound this last term, we note that $R_h$ yields pointwise evaluations, so for any continuous function $\eta$ it will hold that
        $$ \| R_h \eta \|_{\infty,h} \leq \| \eta \|_0, $$
        where $\| \eta \|_0$ is the supremum norm. Finally, we use that $\mathcal A$ is a continuous bijection, and thus from the open mapping theorem it holds that $\|\mathcal A^{-1}\|$ is bounded by some constant $C_2$. Using this fact, the previous estimate becomes 
        $$ \| \mat A_h^{-1} \vec f_h \|_{\infty,h} \leq Ch^k + \| \mathcal A f \|_0 \leq C h^k + C_2 \| \vec f_h \|_0<\infty, $$
        which in particular shows that $ \| \mat A_h^{-1} \vec f_h \|$ is bounded for all $\vec f_h$. The uniform boundedness principle them yields that the operator $\mat A_h^{-1}$ is bounded, which concludes the proof.
    \end{proof}
\end{theorem}

\subsection{Stability of the discrete Laplacian in 1D}
Let us now study the stability of the discretized Laplacian operator in 1D, $-\Delta u = -u''$, over the interval $[a,b]$ with $a<b$ and homogeneous boundary conditions $u_h(x_1) = u_h(x_N) = 0$. Consider $\Omega_h = \{x_i\}_{i=1}^N$ a grid of $[a,b]$ with $N$ equispaced elements, where $x_1=a$ and $x_{N-1}=b$, and mesh size $h=(b-a)/N$. Recall that the numerical second derivative can be approximated by the three-point approximation derived above, which for a given interior point is given by
$$-u_h''(x_i) = -\left(\frac{u_h(x_{i-1}) - 2u_h(x_i) + u_h(x_{i+1})}{h^2}\right) = \frac{1}{h^2}(-u_h(x_{i-1}) + 2u_h(x_i) - u_h(x_{i+1})).$$
We note that in our discretized system we do not need to solve for $u_h(x_1)=u_h(x_N)=0$ because of the boundary condition. This implies that the derivative at $x_2$ and $x_{N-1}$ is
\begin{align*}
    -u_h''(x_2) &= \frac{1}{h^2}(\cancel{-u_h(x_{1})} + 2u_h(x_2) - u_h(x_{3})) = \frac{1}{h^2}(2u_h(x_2) - u_h(x_{3}))\\
    -u_h''(x_{N-1}) &= \frac{1}{h^2}(-u_h(x_{N-2}) + 2u_h(x_{N-1}) - \cancel{u_h(x_{N})}) = \frac{1}{h^2}(-u_h(x_{N-2}) + 2u_h(x_{N-1})),\\
\end{align*}
and thus the finite difference operator associated with $-\Delta$ is
$$A_h = \frac{1}{h^2} \begin{bmatrix} 2 & -1 & 0 & \cdots & 0 \\ -1 & 2 & -1 & \cdots & 0 \\ 0 & -1 & 2 & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & 2\end{bmatrix}.$$
where we have $-\Delta u \approx A_h u_h$. To show stability, we will use that it actually satisfies a discrete maximum principle. 
\begin{theorem}[Discrete maximum principle (DMP)]
    Let $A_h$ be the finite difference operator defined above. Then, for any grid function $u_h$ such that $A_h u_h \geq 0$, it holds that
    $$ \max_{x_i\in \Omega_h} u_h \leq \max_{x_i\in \Gamma_h} u_h, $$
    where $x_i$ is an interior node of $\Omega_h$ and the equality holds if and only if $u_h$ is constant.
    \begin{proof}
        We proceed by contradiction, so we assume that $u_h(x_i) \overset{(*)}{=} \max_{x\in \Omega_h} u_h > \max_{x\in \Gamma_h} u_h$, where this maximum is achieved at the internal node $x_i\in \Omega_h$. Then, from the centered difference scheme we get
        $$2u_h(x_i) = u_h(x_{i-1}) + u_h(x_{i+1}) - h^2 \underbrace{A_h u_h(x_i)}_{\geq 0} \leq u_h(x_{i-1}) + u_h(x_{i+1}) \overset{(*)}{\leq} 2u_h(x_i),$$
        which implies that $u_h(x_i)$ is constant in the discrete neighborhood of $x_i$. Repeating this argument recursively for all interior points, we conclude that $u_h(x_i)$ is constant in $\Omega_h$, which contradicts the assumption. Hence, the discrete maximum principle holds.
    \end{proof}
\end{theorem}
We can apply this principle to show that $A_h$ is stable. 
\begin{theorem}
    Let $\Omega$ be a domain with boundary $\Gamma$, and $A_h$ the finite difference operator associated to the problem
    $$\begin{aligned}
        -\Delta u &= f & \tin \Omega, \\
        u &= g & \ton \Gamma.
    \end{aligned}
    $$
    Then, if $A_h u_h = f_h$ in $\Omega_h$ and $u_h = g_h$ in $\Gamma_h$, it holds that
    $$\|u\|_{\infty,h} \leq C\left(\|f\|_{\infty, h} + \|g\|_{\infty, h}\right),$$
    where we write $\|g\|_{\infty, h} = \sup_{x_i\in\Gamma_h}|g(x_i)|$. 
    \begin{proof}
        We will prove the theorem over $\R$. Using the comparison function $\phi(x) = \frac{c}{2}x^2$ where $c=\|f\|_{\infty, h}$ and consider a constant $M$ such that $\phi(x) \leq cM$ in $\Omega$. Using that $A_h \phi_h = c$ (inspired by $\phi''(x) = c$), we get
        $$A_h(u_h + \phi_h) = A_h u_h + c = f_h + c \geq 0,$$
        and using the discrete maximum principle (DMP) we obtain
        $$\max_{\Omega_h} u_h \leq \max_{\Omega_h} (u_h + \phi_h) \overset{\text{DMP}}{\leq} \max_{\Gamma_h} (u_h + \phi_h) = \|g_h\|_{\infty, h} + cM = \|g_h\|_{\infty, h} + M \|f\|_{\infty, h},$$
        and repeating the argument for $-u_h$ yields the lower bound for $\|u_h\|_{\infty,h}$. This shows that $A_h$ is stable.
    \end{proof}
\end{theorem}

\subsection{Discretization in 2D and 3D}
In this section we hint on how to address numerically higher dimensional problems, and also how to include time. The stability of such schemes will be studied further ahead with the von Neumann stability analysis. In typical 1D examples, one has a domain $(a,b)$ and an associated grid given by some points $\{x_i\}_i^N$ such that each index $i$ corresponds to an order of the domain, and thus naturally a finite difference operator is going to involve something like $i$ and its neighbors. This scenario would be different if, for example, one had a random ordering of the grid points such as
    $$ x_0 = a,\; x_7 = a + h,\; x_{12} = a + 2h,\; \hdots, x_N = b, $$
so that in such a grid, a forward difference with respect to the point $x_0$ would be given by
    $$ D^+f(x_0) = \frac 1 h \left( f^7 - f^0 \right). $$
Although this is an unnatural way to do things in 1D, it is the natural scenario arising in higher dimensions. For this, and for the sake of simplicity, we will restrict the presentation to meshes given by tensor products of intervals, i.e. $\Omega = (a_x, b_x) \times (a_y, b_y)$ in 2D and $\Omega = (a_x, b_x) \times (a_y, b_y) \times (a_z, b_z)$ in 3D. The main difficulty will be that of creating an ordering of the degrees of freedom.

Let us now consider the 2D case. Consider $\Omega = (a_x, b_x) \times (a_y, b_y)$ and an equispaced discrete grid $\Omega_h$ with $ a_x = x_0, \hdots, x_{N_x-1} = b_x$ and $a_y = y_0, \hdots, b_y = y_{N_y-1}$, where $N_x$ and $N_y$ are number of points in each subdomain. Then, each grid point will be given by     
    $$ \vec X_{ij} \coloneqq (x_i, y_j) \qquad \forall i,j \in \{0,\hdots, N_x-1\}\times \{0,\hdots, N_y-1\}, $$
and analogously we can define a (matrix) grid function associated to some continuous function $f:\Omega\to \R$ as 
$$ \vec f^{ij} \coloneqq f(\vec X_{ij}) = f(x_i, y_j) \in \R^{N_x}\times \R^{N_y} $$
We can flatten 2D matrices to obtain 1D vectors through the definition of an \textit{index map} $I: \{0,\hdots, N_x-1\} \times \{0, \hdots, N_y-1\}\to \{0,\hdots, N_xN_y-1\}$ given by 
    $$ I \coloneqq I(i,j) = i + j N_x.$$
Fortunately, this map can be inverted as
    $$ I \mapsto (i(I), j(I)) \coloneqq (I\mod N_x, \lfloor I/N_x \rfloor), $$
and so we can uniquely define the 1D vector $\vec f^I$ as the vector of all grid points in $\Omega$ as
    $$ \vec f^I \coloneqq \vec f^{i(I)j(I)} = f(\vec X_{i(I)j(I)}) \in \R^{N_xN_y}. $$
For example, if one has a grid with $N_x=3$ and $N_y=2$, then the forward and inverse index maps are as follows:
    \begin{table}[ht!]
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
        \begin{tabular}{c c | c}
            \toprule $i$ & $j$ & $I(i,j)$ \\ \midrule
            0 & 0 & 0 \\
            1 & 0 & 1 \\
            2 & 0 & 2 \\
            0 & 1 & 3 \\
            1 & 1 & 4 \\
            2 & 1 & 5 \\ \bottomrule
        \end{tabular}
        \caption{Forward map}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
        \begin{tabular}{c | c c}
            \toprule $I$ & $i(I)$ & $j(I)$ \\ \midrule
            0 & 0 & 0 \\
            1 & 1 & 0 \\
            2 & 2 & 0 \\
            3 & 0 & 1 \\
            4 & 1 & 1 \\
            5 & 2 & 1 \\ \bottomrule
        \end{tabular}
        \caption{Backward map}
        \end{subfigure}
        \caption{Forward and inverse index maps for a 2D example.}
    \end{table}

Using these maps, we define a forward difference in $x$ and $y$ directions as $D^+_xf(\vec X)$ and $D^+_yf(\vec X)$ and note that they are given by
    \begin{align*}
        D^+_x f(\vec X_{ij}) = \frac 1{h_x} \left(\vec f^{(i+1)j} - \vec f^{ij}\right)= \frac 1 {h_x} \left( \vec f^{I(i+1,j)} - \vec f^{I(i,j)}\right), \\
        D^+_y f(\vec X_{ij}) = \frac 1{h_y} \left(\vec f^{i(j+1)} - \vec f^{ij}\right) = \frac 1{h_y} \left(\vec f^{I(i,j+1)} - \vec f^{I(i,j)}\right),
    \end{align*}
where we have used a slight abuse of notation by denoting both the 2D vector and 1D vectors as $\vec f^{ij}$ and $\vec f^I$ respectively. Further, the second derivative operators can be defined as 
\begin{align*}
    D_{xx} f(\vec X_{ij}) &= \frac{-1}{h_x^2} \left( -\vec f^{(i+1)j} + 2\vec f^{ij} - \vec f^{(i-1)j}\right) = \frac{-1}{h_x^2} \left( -\vec f^{I(i+1,j)} + 2\vec f^{I(i,j)} - \vec f^{I(i-1,j)}\right) = A_h f(\vec X_{ij}), \\
    D_{yy} f(\vec X_{ij}) &= \frac{-1}{h_y^2} \left( -\vec f^{i(j+1)} + 2\vec f^{ij} - \vec f^{i(j-1)}\right) = \frac{-1}{h_y^2} \left( -\vec f^{I(i,j+1)} + 2\vec f^{I(i,j)} - \vec f^{I(i,j-1)}\right) = f(\vec X_{ij})A_h^\top,
\end{align*}
where $A_h$ is the discrete second derivative matrix. Let us now apply this for the Laplace equation with zero boundary conditions, that is,
$$\begin{aligned}
    -\Delta u &= f & \tin \Omega, \\
    u &= 0 & \ton \partial\Omega.
\end{aligned}$$
Here, the corresponding discrete equation is
\begin{equation*}
    -\Delta u(\vec X_{ij}) = - \frac{\partial^2}{\partial x^2}\vec u(X_{ij}) - \frac{\partial^2}{\partial y^2} u(\vec X_{ij}) \approx A_h u(\vec X_{ij}) + u(\vec X_{ij})A_h^\top  = f(\vec X_{ij}),
\end{equation*}
which is a matrix equation since $u(\vec X_{ij})$ is a grid function. We define a suitable index map $I$, which allows us to write the 1D vectors $u^I$ and $f^I$, and now it remains to define the operator matrix for these 1D vectors. For this, we use the Kronecker product $\otimes$ defined by
$$
\begin{aligned}
    \otimes : \R^{m\times n} \times \R^{p\times q} &\to \R^{mp\times nq}, \\
    (A,B) &\mapsto A\otimes B \coloneqq \begin{bmatrix}
        A_{11}B & A_{12}B & \cdots & A_{1n}B \\
        A_{21}B & A_{22}B & \cdots & A_{2n}B \\
        \vdots & \vdots & \ddots & \vdots \\
        A_{m1}B & A_{m2}B & \cdots & A_{mn}B
    \end{bmatrix}. \\
\end{aligned}
$$
This operation is needed to correctly define the operator matrix. For this, we need a brief lemma to calculate this matrix. 
\begin{lemma}
    Let $A\in R^{m\times n}$, $B\in \R^{p\times q}$ and $X\in \R^{q\times n}$. Denote the vector form of a matrix (in our case, a grid function) $A$ as $\vec A^I$, $A_i\in \R^n$ the rows of $A$ and $X_i\in \R^q$ the columns of $X$. Then, 
    $$(BXA^\top)^I = (A\otimes B)\vec X^I$$.
    \begin{proof}
        We expand from the right hand side:
        \begin{align*}
            (A\otimes B)\vec x^I &= \begin{bmatrix}
                A_{11}B & A_{12}B & \cdots & A_{1n}B \\
                A_{21}B & A_{22}B & \cdots & A_{2n}B \\
                \vdots & \vdots & \ddots & \vdots \\
                A_{m1}B & A_{m2}B & \cdots & A_{mn}B
            \end{bmatrix} \begin{bmatrix}
                X_1^\top \\
                X_2^\top \\
                \vdots \\
                X_n^\top
            \end{bmatrix} \\
            &= \begin{bmatrix}
                \sum_{i=1}^n A_{1i} B X_i\\
                \sum_{i=1}^n A_{2i} B X_i\\
                \vdots \\
                \sum_{i=1}^n A_{mi} B X_i
            \end{bmatrix}\\
            &= \begin{bmatrix}
                BXA_1^\top\\
                BXA_2^\top\\
                \vdots \\
                BXA_m^\top\\
            \end{bmatrix}\\
            &= (BXA^\top)^I.
        \end{align*}
    \end{proof}
\end{lemma}
Now, we take the 1D vector form of the discrete Laplace equation and invoke the above lemma with $A=I$ the identity matrix:
\begin{align*}
    \vec f^I &= (A_h u(\vec X_{ij}) + u(\vec X_{ij})A_h^\top)^I\\
    &= (A_h u(\vec X_{ij}) I)^I + (I u(\vec X_{ij})A_h^\top)^I\\
    &= (I\otimes A_h)\vec u^I + (A_h^\top \otimes I)\vec u^I\\
    &= (A_h\otimes I + I\otimes A_h)\vec u^I \tag{$A_h$ symmetric}.
\end{align*}
Thus, our new operator is now $D^2_h = A_h\otimes I + I\otimes A_h$, and then the discrete Laplace equation is reduced to the linear system $D^2_h \vec u^I = \vec f^I$, where $D^2_h$ is sparse by construction. We can use the same methods as before to prove the stability of the scheme.  

\subsection{Time discretization}
Time discretization is an enormous topic, so here we provide simple strategies and ways to think about them. We will dedicate an entire section to their analysis using the von Neumann stability analysis. To avoid messing with Bochner spaces, we will refrain from considering a precise functional setting, and leave this for further ahead in the notes. Consider a differential operator $\mathcal L$ and its associated discrete matrix $\mat L_h$ which includes the problems boundary conditions. The continuous differential equation, for some initial condition $x(0) = x_0$, is given by 
    $$ \dot x + \mathcal L x = f, $$
    which we can discretize in space to obtain the system of differential equations
    $$ \dot x_h + \mat L_h \vec x_h = \vec f_h. $$
    Instead of considering a Taylor approximation of the time derivative, we will use a quadrature rule to obtain different strategies. For this, integrating in the interval $(t^n, t^{n+1})$ and using the notation $\vec x(t^n) \approx \vec x^{n}$, yields
    $$ \vec x^{n+1} - \vec x^n + \int_{t^n}^{t^{n+1}} \mat L_h \vec x_h(s) \,ds = \int_{t^n}^{t^{n+1}} \vec f_h(s)\,ds. $$

    We present three different schemes depending on three different quadrature rules: 
    \begin{itemize}
        \item Left-sided rule (Explicit):
                $$ \int \mat L_h \vec x_h(s)\,ds \approx \Delta t \mat L_h\vec x_h^n. $$
        \item  Right-sided rule (Implicit): 
            $$ \int \mat L_h \vec x_h(s)\,ds \approx \Delta t\mat L_h\vec x_h^{n+1}. $$
        \item  Trapezoidal rule (Mid-point): 
            $$ \int \mat L_h \vec x_h(s)\,ds \approx \frac{\Delta t}{2}\left(\mat L_h\vec x_h^{n+1}+ \mat L_h \vec x_h^n\right). $$
    \end{itemize}
    The commonly acknowledged relevant points here are: 
    \begin{itemize}
        \item The explicit scheme yields a system that is easy to solve but unstable.
        \item The implicit scheme is harder to solve but more (usually unconditionally) stable.
        \item The mid-point or \emph{Crank-Nicholson} scheme is also stable, with better accuracy but involves more computations and possible numerical saturation at small timesteps.
    \end{itemize}

    A natural generalization of these three schemes is the $\theta$-method, which for a fixed parameter $\theta\in[0,1]$, results in the quadrature rule
    $$\int \mat L_h \vec x_h(s)\,ds \approx \theta\Delta t \mat L_h\vec x_h^{n+1} + (1-\theta)\Delta t \mat L_h \vec x_h^n. $$
    We note that $\theta=1$ corresponds to the explicit scheme, $\theta=0$ to the implicit scheme, and $\theta=1/2$ to the mid-point scheme. 
    
\subsection{Von Neumann stability analysis}
Energy methods for stability analysis can quickly become untractable, so in order to get easier bounds, we can perform this analysis after moving the equation to the frequency domain. This is the so-called \emph{von Neumann stability analysis}, which is a very powerful tool for analyzing the stability of finite difference schemes. The idea is to assume that the solution can be expressed as a Fourier series, and then analyze the growth of the Fourier modes over time.

For this analysis, we will use the Fourier transform, which is comfortable for this task as it yields simple eigenfunctions of the derivatives. 

\begin{definition}[Fourier transform]
    Let $v\in L^2(\Omega)$. Then, its Fourier transform $\mathcal{F}(v) := \hat{v}$ is defined as
    $$ \mathcal{F}(v)(\xi) := \hat{v}(\xi) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} v(x)e^{-i\xi x}dx, $$
    where $\xi$ is the frequency variable and $i=\sqrt{-1}$. This transform satisfies the Parseval identity:
    $$ \| v\|_{L^2} = \| \hat{v}\|_{L^2}, $$
    and the inverse transform is defined as
    $$ v(x) = \mathcal{F}^{-1}(\hat{v})(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty} \hat{v}(\xi)e^{i\xi x}d\xi. $$
    The most important identity we will use is the Fourier transform of the derivative, which is given by
    $$ \mathcal{F}(v')(x) = i\xi \hat{v}(\xi). $$
    This definition is mostly relevant for differential operators over $\R$, so it will only give guidelines for actual problems.
\end{definition}
Let us consider a basis function $e^{i\xi x}$ for a given wave number $\xi$, and for some finite difference scheme, its grid function
$$W_j = e^{i\xi (jh)}.$$
This function will be an eigenfunction of all translation-invariant finite difference operators. Set $x_j = jh$, then
$$D^+ W(x_j) = \frac{1}{h}\left(e^{i\xi (j+1)h} - e^{i\xi j h}\right) = \frac{1}{h}\left(e^{i\xi h} - 1\right)e^{i\xi j h} = \frac{e^{i\xi h} - 1}{h}W_j.$$
This helps in decoupling the degrees of freedom, and by the Parseval identity, we can again try to get a bound of the form
$$\|u^{n+1}\| = \|\hat{u}^{n+1}\|\leq |g(\xi)| \|\hat{u}^{n}\| = |g(\xi)| \|u^n\|,$$
where $g(\xi)$ is defined as an amplification factor. This scheme will be stable when $|g(\xi)|\leq 1$.

Let us now illustrate the application on the heat equation. Consider the boundary value problem
$$ \begin{aligned}
    \dot{u}(x,t) - u_{xx}(x,t) &= 0, &\quad (x,t) &\in \Omega \times (0,T), \\
    u(x,0)                  &= u^0(x), &\quad x &\in \Omega, \\
    u(x,t)                  &= f(x,t), &\quad (x,t) &\in \partial\Omega \times (0,T).
\end{aligned} $$
With the finite finite difference scheme we get
$$ \frac{u^{n+1}_j - u^n_j}{\Delta t} + \frac{1}{h^2} (-u^n_{j+1} +2u^n_j - u^n_{j-1}) = 0.$$
Here we set $u_j^n = e^{i\xi (jh)}$, and set the ansatz $u_j^{n+1} = g(\xi) u_j^n = g(\xi) e^{i\xi (jh)}$. Inserting this into the scheme we get
\begin{align*}
g(\xi) e^{i\xi j h} &= e^{i\xi (jh)} - \frac{\Delta t}{h^2}\left(-e^{i\xi (j-1) h} + 2e^{i\xi (jh)} - e^{i\xi (j+1) h}\right)\\
&= \left(1 + \frac{\Delta t}{h^2}\left(e^{-i\xi h} - 2 + e^{i\xi h}\right)\right)e^{i\xi(jh)}.
\end{align*}
We thus have
\begin{align*}
g(\xi) &= 1 + \frac{\Delta t}{h^2}\left(e^{-i\xi h} + e^{i\xi h}- 2 \right) \\
&= 1 + \frac{\Delta t}{h^2}\left(2\cos(\xi h) - 2\right) \tag{$e^{-i\xi h} + e^{i\xi h} = 2\cos(\xi h)$} \\
&= 1 - \frac{2\Delta t}{h^2}\left(1 - \cos(\xi h)\right).
\end{align*}
With this form, we can use the fact that $\cos(\xi h) \in (-1,1)$, which yield the following two bounds:
\begin{align*}
    g(\xi) &\leq 1 + \frac{2\Delta t}{h^2}(1-1) = 1 \\
    g(\xi) &\geq 1 + 2\frac{\Delta t}{h^2}(-1-1)  = 1 - \frac{4\Delta t}{h^2}.
\end{align*}
We note that the first inequality yields $g(\xi)\leq 1$, and from the second inequality we seek $-1\leq g(\xi)$. Bounding by below, we get
\begin{align*}
    1-4\frac{\Delta t}{h^2} \geq -1 &\iff 2\geq 4\frac{\Delta t}{h^2}\\
    &\iff \Delta t \leq \frac{h^2}{2},
\end{align*}
which yields an upper bound for the time step $\Delta t$ to ensure stability of the finite difference scheme. Inequalities of this kind are studied more thoroughly in \cite{LeVeque2007}.

\subsection{Stability in time}
We now look for stability estimates regarding also time. We first study a forward scheme 
$$
\begin{cases}
    \frac{u^{n+1}-u^n}{\Delta t} &= cu^n\\
    u^0&= u_{h,0},
\end{cases}
$$
with $c\in \R$. We note that a result such as $\|u^{n+1}\|\leq c\|u^n\|$ for $c\leq 1$ results in $\|u_h\|_{\infty, h} \leq \|u^0\|_{\infty, h}$. This is indeed a stability result, so it will be common tu study equations in the form $u^{n+1} = g(u^n)$. Given that our theory works for linear operators, we will start by studying a simple first-order equation:
$$
\begin{cases}
    u'+cu &= f\\
    u(0) &= u_0.
\end{cases}
$$
Now we look at the stability of forward and backward Euler and the $\theta$-method. 
\begin{enumerate}
    \item \underline{Explicit}: we have $u^{n+1}-u^n + c\Delta t u^n = \Delta t f^n$. Stability is a property of the differential operator, and thus we analyze it with $f\equiv 0$. We note that
    $$u^{n+1} = (1-c\Delta t)u^n \implies |u^{n+1}|\leq |1-c\Delta t||u^n|,$$
    and thus this scheme is stable if $|1-c\Delta t|< 1$. We note two cases:
    \begin{itemize}
        \item If $c > 0$, then we have $|1-c\Delta t|<1$ if $\Delta t < \frac{2}{c}$. This means that this method is \underline{conditionally stable}. 
        \item If $c< 0$, then $|1-c\Delta t|\geq 1$, which means that the scheme is \underline{unconditionally unstable}.
    \end{itemize} 
    \item \underline{Implicit}: we have $u^{n+1}-u^n + c\Delta tu^{n+1} = \Delta t f^n$. This yields
    $$(1+c\Delta t) u^{n+1} = u^n \implies |u^{n+1}| \leq \left|\frac{1}{1+c\Delta t}\right| |u^{n}|.$$
    \begin{itemize}
        \item If $c > 0$,  we have $1+c\Delta t >1$, which means that the method is \underline{unconditionally stable}. 
        \item If $c < 0$, then $1+c\Delta t <1$, so the scheme is \underline{unconditionally unstable}.
    \end{itemize}     
    It is important to note that if $c<0$, then the solution is just $u(x) = Ce^x$, so a condition such as $|u^{n+1}|<|u^n|$ does not make sense. Thankfully, our analysis reveals that, and we now just assume $c>0$. 
    \item \underline{$\theta$-method}: we have
    $$
    u^{n+1}-u^n + \theta c\Delta t u^{n+1} + (1-\theta)c\Delta t u^n = 0.
    $$
    Rearranging this we get
    $$
    (1+\theta c\Delta t)u^{n+1} = (1- (1-\theta)c\Delta t) u^n \implies |u^{n+1}| \leq \left|\frac{1-(1-\theta)c\Delta t}{1+\theta c\Delta t}\right| |u^n|.
    $$
    We now recognize two cases. 
    \begin{itemize}
        \item If $\Delta t$ is big and such that $1-(1-\theta)c\Delta t < 0$, we have
        \begin{align*}
            -\frac{1-(1-\theta)c\Delta t}{1+\theta c\Delta t} \leq 1 &\iff -1+(1-\theta)c\Delta t \leq 1+\theta c\Delta t\\
            &\iff (1-2\theta) c\Delta t \leq 2\\
            &\iff \left(\frac{1}{2}-\theta\right)c\Delta t\leq 1.
        \end{align*}
        So, if $\theta\geq 1/2$, the method is unconditionally stable, and if $\theta <1/2$, the method is \emph{conditionally stable}. 
        \item If $\Delta t$ is small, such that $1-(1-\theta)c\Delta t < 1 + \theta c \Delta t$, then we get $-c\Delta t < 1$, which is \emph{unconditionally stable}. 
    \end{itemize}
\end{enumerate}

\subsection{Unavoidable nonlinearities}
Sometimes, specially in stationary models, we cannot avoid having to solve a nonlinear problem. Two common ways to obtain an approximate solution to a nonlinear problem are 1) fixed point methods (usually of first order), and 2) Newton-Raphson methods. To analyze them, consider the domain $\Omega=(a,b)$ and the example problem
$$
\begin{cases}
    -u'' + \sin(u) &= 0\\
    u(a) &= u^a\\
    u(b) &= u^b.
\end{cases}
$$
As usual, we set a discrete grid of $N$ equispaced interior points $a=x_1<\cdots<x_N=b$. Thus, at each non-boundary node $x_i$ we have the equation
$$
-\frac{1}{h^2}(u^{i+1}-2u^i+u^{i-1}) + \sin(u^i) = 0.
$$
We write $g(\vec u_h) = \{g(u^i)\}$ for the vector of evaluations of $g$ at the non-boundary nodes. This allows us to write more compactly the root-finding problem
$$F(\vec u_h):= \mat A_h \vec u_h + \sin(\vec u_h) = 0.$$
Let us analyze both methods for solving this problem.
\begin{enumerate}
    \item \underline{Newton-Raphson}: we look for solutions of a linearized problems and iterate. Abstractly, we solve the equation $F(\vec x)=0$ by considering an initial point $\vec x^0$ and then using Taylor's theorem for linearizing $F(\vec x^{k+1})$ (unknown) around the previous iteration result $\vec x^k$ (known):
    $$
    F(\vec x^{k+1}) \approx F(\vec x^k) + \nabla F(\vec x^k)\cdot \underbrace{\vec{\delta x}^{k+1}}_{\vec x^{k+1} - \vec{x}^k}.
    $$
    We can now solve a system for $\vec{\delta x}^{k+1}$ such that $F(\vec x^{k+1}) = 0$, that is,
    $$\underbrace{[\nabla F(\vec x^k)]}_{\text{matrix}} \underbrace{\vec{\delta x}^{k+1}}_{\text{vector}} = \underbrace{-F(\vec x^k)}_{\text{vector}},$$
    along with the update step
    $$\vec x^{k+1} = \vec{x}^k + \vec{\delta x}^{k+1}.$$
    We call such a system a \textit{Newton iteration} or \textit{tangent system}. Also, $\vec{\delta x}^{k}$ must have homogeneous (zero) boundary conditions, so that all $\{\vec x^{k}\}_k$ satisfy the original ones. 
    
    In our problem, the $i$-th component of $F(\vec u_h)$ is given by
    $$
    [F(\vec u_h)]^i = -\frac{1}{h^2}(u^{i-1}-2u^i+u^{i+1}) + \sin(u^i),
    $$
    which we differentiate to get
    $$
    [\nabla F(\vec u_h)]_{ij} = \frac{\partial [F(\vec u_h)]^i}{\partial u^j} = \begin{cases}
        -\frac{1}{h^2} &j\in\{i-1,i+1\}\\
        \frac{2}{h^2} + \cos(u^i) &j=i\\
        0 &\text{elsewhere.}
    \end{cases}
    $$
    We can write more compactly this gradient as
    $$
    \nabla F(\vec u_h) = \mat A_h + \textbf{diag}(\cos(\vec u_h)),
    $$
    which allows us to write the tangent system as
    $$
    \left(\mat A_h + \textbf{diag}(\cos(\vec u^k))\right)\vec{\delta u}^{k+1} = -\left(\mat A_h \vec{u}^k + \textbf{diag}(\sin(\vec u^k))\right).
    $$
    \item \underline{Fixed point}: similarly to the time-dependent case, we can \textit{delay} the evaluation of a nonlinearity. We do this by choosing a starting point $\vec u^0$ and evaluating the nonlinear part in the previous iteration $\vec u^k$. This yields
    $$
    -\nabla \vec u^{k+1} + \sin(\vec u^k) = 0.
    $$
    Note that we can enforce this equality at the discrete level as well, i.e. 
    $$
    -\nabla \vec u_h^{k+1} + \sin(\vec u_h^k) = 0,
    $$
    where $\vec u_h^k$ is the discrete vector $\vec u_h$ at the $k$-th iteration. In such cases, we can sometimes prove the convergence of the scheme. For that, we consider the discrete iterates $\vec u_h^k$ given by
    \begin{align*}
        \mat A_h \vec u_h^{k+1} + \sin(\vec u_h^k) &= 0
        \mat A_h \vec u_h^{k} + \sin(\vec u_h^{k-1}) &= 0,
    \end{align*}
    which subtracted give the equation
    $$
    \mat A_h(\vec u_h^{k+1} - \vec u_h^{k}) = -(\sin(\vec u_h^k) - \sin(\vec u_h^{k-1})).
    $$
    We typically call this equation an \textit{error equation}. Finally, we can use the fact that $\mat A_h$ is stable, so we can bound the error norm as
    \begin{align*}
        \|\vec u_h^{k+1} - \vec u_h^{k}\|_{\infty, h} &\leq C\|\mat A_h (\vec u_h^{k+1} - \vec u_h^{k})\|_{\infty, h} \tag{Stability}\\
        &= C\|-(\sin(\vec u_h^k) - \sin(\vec u_h^{k-1}))\|_{\infty, h}\tag{Error equation}\\
        &\leq C\|\vec u_h^{k+1} - \vec u_h^{k}\|_{\infty, h},\tag{$\sin$ is $1$-Lipschitz}
    \end{align*}
    thus the scheme is convergent if $C<1$. Otherwise, the scheme \underline{can converge}, but we have no theoretical guarantee from this analysis. Still, each fixed point iteration
    $$
    \mat A_h \vec u_h^{k+1} = - \sin(\vec u_h^k)
    $$
    is much simpler than a Newton iteration
    \begin{align*}
        \left(\mat A_h + \textbf{diag}(\cos(\vec u^k))\right)\vec{\delta u}^{k+1} &= -\left(\mat A_h \vec{u}^k + \sin(\vec u^k)\right)\\
        \vec u_h^{k+1} &= \vec{u}^k_h + \vec{\delta u}_h^{k+1}.
    \end{align*}
\end{enumerate}
Both fixed point and Newton-Raphson methods are commonly used strategies, and in general there is no universally better choice. We study nonlinear problems more thoroughly in Chapter~\ref{chapter:nonlinear}.


